{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/inputtransformer2.py:468: UserWarning: `make_tokens_by_line` received a list of lines which do not have lineending markers ('\\n', '\\r', '\\r\\n', '\\x0b', '\\x0c'), behavior will be unspecified\n",
      "  warnings.warn(\"`make_tokens_by_line` received a list of lines which do not have lineending markers ('\\\\n', '\\\\r', '\\\\r\\\\n', '\\\\x0b', '\\\\x0c'), behavior will be unspecified\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.8442}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"Ironhack is a Global Tech School ranked num 2 worldwide.  ",
    " ",
    "Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do.\"\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "analyzer.polarity_scores(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGA DEL DATASET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.read_csv('./Sentiment140.csv/Sentiment140.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsample = sentiment.sample(n=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES PARA EL CLEANING DEL TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(s):\n",
    "\n",
    "    s = re.sub(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', s)\n",
    "\n",
    "    s = re.sub('[^a-zA-Z_]+', ' ', s)\n",
    "\n",
    "    s = s.lower()\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    \"\"\"\n",
    "    Tokenize a string.\n",
    "\n",
    "    Args:\n",
    "        s: String to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        A list of words as the result of tokenization.\n",
    "    \"\"\"\n",
    "    return word_tokenize(s)\n",
    "\n",
    "\n",
    "def stem_and_lemmatize(l):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in l:\n",
    "\n",
    "        sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "        x = sno.stem(i)\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        x = lemmatizer.lemmatize(x)\n",
    "\n",
    "        result.append(x)\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_stopwords(l):\n",
    "\n",
    "    stop_words = get_stop_words('en')\n",
    "\n",
    "    filtered_words = [word for word in l if word not in stop_words]\n",
    "\n",
    "    return filtered_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUCIÓN DE FUNCIONES DE LIMPIEZA DE TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsample['text_processed'] = dfsample['text'].apply(clean_up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsample['text_processed'] = dfsample['text_processed'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsample['text_processed'] = dfsample['text_processed'].apply(stem_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsample['text_processed'] = dfsample['text_processed'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>771198</th>\n",
       "      <td>0</td>\n",
       "      <td>2302274066</td>\n",
       "      <td>Tue Jun 23 16:34:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>benwablz</td>\n",
       "      <td>Just returned from Funroe...not looking forwar...</td>\n",
       "      <td>[just, return, funro, look, forward, go, back,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339690</th>\n",
       "      <td>0</td>\n",
       "      <td>2014692444</td>\n",
       "      <td>Wed Jun 03 02:22:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>adeyus</td>\n",
       "      <td>needs to open up a lil bit more... sorry baby</td>\n",
       "      <td>[need, open, lil, bit, sorri, babi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114455</th>\n",
       "      <td>0</td>\n",
       "      <td>1826305009</td>\n",
       "      <td>Sun May 17 08:11:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lauren574</td>\n",
       "      <td>is going to her grandpas 75th birthday party (...</td>\n",
       "      <td>[go, grandpa, th, birthday, parti, haha, inste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257744</th>\n",
       "      <td>0</td>\n",
       "      <td>1985035567</td>\n",
       "      <td>Sun May 31 16:17:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ErikaBarros</td>\n",
       "      <td>@taylorswift13 i dont have nbc.. i live in bra...</td>\n",
       "      <td>[taylorswift, dont, nbc, live, brazil, want, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68286</th>\n",
       "      <td>0</td>\n",
       "      <td>1692776723</td>\n",
       "      <td>Sun May 03 20:45:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>wondrous_as_u</td>\n",
       "      <td>My B Bear is not himself.  Nothing so sad as a...</td>\n",
       "      <td>[b, bear, noth, sad, puppi, s, dump, s, probab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date      flag  \\\n",
       "771198       0  2302274066  Tue Jun 23 16:34:46 PDT 2009  NO_QUERY   \n",
       "339690       0  2014692444  Wed Jun 03 02:22:16 PDT 2009  NO_QUERY   \n",
       "114455       0  1826305009  Sun May 17 08:11:25 PDT 2009  NO_QUERY   \n",
       "257744       0  1985035567  Sun May 31 16:17:56 PDT 2009  NO_QUERY   \n",
       "68286        0  1692776723  Sun May 03 20:45:30 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 user                                               text  \\\n",
       "771198       benwablz  Just returned from Funroe...not looking forwar...   \n",
       "339690         adeyus     needs to open up a lil bit more... sorry baby    \n",
       "114455      Lauren574  is going to her grandpas 75th birthday party (...   \n",
       "257744    ErikaBarros  @taylorswift13 i dont have nbc.. i live in bra...   \n",
       "68286   wondrous_as_u  My B Bear is not himself.  Nothing so sad as a...   \n",
       "\n",
       "                                           text_processed  \n",
       "771198  [just, return, funro, look, forward, go, back,...  \n",
       "339690                [need, open, lil, bit, sorri, babi]  \n",
       "114455  [go, grandpa, th, birthday, parti, haha, inste...  \n",
       "257744  [taylorswift, dont, nbc, live, brazil, want, b...  \n",
       "68286   [b, bear, noth, sad, puppi, s, dump, s, probab...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREACIÓN DEL BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for x in dfsample.text_processed:\n",
    "    words += x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(words)\n",
    "\n",
    "voc = fdist.most_common(5000)\n",
    "\n",
    "bag_of_words = [x[0] for x in voc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREACIÓN DEL DICCIONARIO PARA APLICAR nltk.NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in bag_of_words:\n",
    "        features[w] = (w in words)\n",
    "    s = SentimentIntensityAnalyzer().polarity_scores(\" \".join(document))\n",
    "    if s[\"pos\"] > 0.2:\n",
    "        s = True\n",
    "    else:\n",
    "        s = False\n",
    "    return list((features, s))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771198     [{'s': False, 't': False, 'go': True, 'just': ...\n",
       "339690     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "114455     [{'s': False, 't': False, 'go': True, 'just': ...\n",
       "257744     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "68286      [{'s': True, 't': False, 'go': False, 'just': ...\n",
       "873475     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "611451     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1239595    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1039038    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1505898    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "324504     [{'s': False, 't': True, 'go': False, 'just': ...\n",
       "602208     [{'s': False, 't': True, 'go': True, 'just': F...\n",
       "1486925    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "995884     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "577495     [{'s': False, 't': False, 'go': True, 'just': ...\n",
       "841638     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "175045     [{'s': False, 't': True, 'go': False, 'just': ...\n",
       "364176     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1106859    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "312760     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "846935     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "393633     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1278872    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "789294     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "813394     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "643521     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "371107     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1553918    [{'s': True, 't': False, 'go': False, 'just': ...\n",
       "270222     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "791332     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "                                 ...                        \n",
       "443214     [{'s': False, 't': False, 'go': True, 'just': ...\n",
       "1327627    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1103006    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "463080     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "626888     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "969444     [{'s': False, 't': False, 'go': True, 'just': ...\n",
       "567374     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "282714     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "472043     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1598105    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "239331     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1250186    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "9495       [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1006366    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1465186    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "907321     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "564282     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "19943      [{'s': False, 't': False, 'go': True, 'just': ...\n",
       "1185123    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1203301    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "597249     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "361985     [{'s': False, 't': True, 'go': False, 'just': ...\n",
       "1206319    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "1006494    [{'s': False, 't': False, 'go': False, 'just':...\n",
       "229138     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "360625     [{'s': True, 't': False, 'go': False, 'just': ...\n",
       "14100      [{'s': False, 't': False, 'go': False, 'just':...\n",
       "857074     [{'s': False, 't': False, 'go': True, 'just': ...\n",
       "482213     [{'s': False, 't': False, 'go': False, 'just':...\n",
       "376890     [{'s': False, 't': True, 'go': False, 'just': ...\n",
       "Name: text_processed, Length: 500, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_in_text = dfsample['text_processed'].apply(find_features)\n",
    "feature_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREACIÓN DE TRAINING Y TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = feature_in_text[:400]\n",
    "testing_set = feature_in_text[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERACIÓN DEL CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 82.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      ha = True             True : False  =     10.6 : 1.0\n",
      "                     lol = True             True : False  =      9.7 : 1.0\n",
      "                    hope = True             True : False  =      8.7 : 1.0\n",
      "                   thank = True             True : False  =      8.5 : 1.0\n",
      "                    well = True             True : False  =      6.9 : 1.0\n",
      "                    like = True             True : False  =      6.9 : 1.0\n",
      "                     yes = True             True : False  =      6.9 : 1.0\n",
      "                    good = True             True : False  =      6.5 : 1.0\n",
      "                    miss = True            False : True   =      5.6 : 1.0\n",
      "                    wish = True             True : False  =      5.2 : 1.0\n",
      "                     way = True             True : False  =      5.1 : 1.0\n",
      "                      na = True            False : True   =      4.5 : 1.0\n",
      "                  better = True             True : False  =      4.1 : 1.0\n",
      "                    hous = True             True : False  =      4.1 : 1.0\n",
      "                       n = True             True : False  =      4.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Question 1: Improve Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizado en un jupyter a parte: Bonus-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Question 2: Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizado en un jupyter a parte: Bonus-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
