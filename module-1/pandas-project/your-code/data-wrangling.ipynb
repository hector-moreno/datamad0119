{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEncontrar la ubicación idónea para aquellas que quieren vivir experiencias extremas\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Título del proyecto\n",
    "\n",
    "'''\n",
    "\n",
    "Shark adventures: ¿Quieres sentir la adrenalina de nadar con tiburones?\n",
    "\n",
    "'''\n",
    "\n",
    "# Objetivo del proyecto\n",
    "\n",
    "'''\n",
    "Encontrar la ubicación idónea para aquellas que quieren vivir experiencias extremas\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la importaciones que necesitaremos a lo largo del proyecto\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el archivo csv\n",
    "\n",
    "dataoriginal = pd.read_csv('GSAF5.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne of the first things we want to do is examine the data and look for any potential issues. Some of the things we are interested in identifying in the data at this stage include:\\n\\n    Missing values\\n    Special characters\\n    Incorrect values\\n    Extreme values or outliers\\n    Duplicate records\\n    Incorrect data types\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guía de trabajo del proyecto\n",
    "'''\n",
    "One of the first things we want to do is examine the data and look for any potential issues. Some of the things we are interested in identifying in the data at this stage include:\n",
    "\n",
    "    Missing values\n",
    "    Special characters\n",
    "    Incorrect values\n",
    "    Extreme values or outliers\n",
    "    Duplicate records\n",
    "    Incorrect data types\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>Species</th>\n",
       "      <th>Investigator or Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.09.18.c</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orlando Sentinel, 9/19/2016</td>\n",
       "      <td>2016.09.18.c-NSB.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.18.c</td>\n",
       "      <td>2016.09.18.c</td>\n",
       "      <td>5993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.09.18.b</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Chucky Luciano</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orlando Sentinel, 9/19/2016</td>\n",
       "      <td>2016.09.18.b-Luciano.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.18.b</td>\n",
       "      <td>2016.09.18.b</td>\n",
       "      <td>5992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.09.18.a</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Orlando Sentinel, 9/19/2016</td>\n",
       "      <td>2016.09.18.a-NSB.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.18.a</td>\n",
       "      <td>2016.09.18.a</td>\n",
       "      <td>5991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.09.17</td>\n",
       "      <td>17-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Thirteenth Beach</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Rory Angiolella</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Age, 9/18/2016</td>\n",
       "      <td>2016.09.17-Angiolella.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.17</td>\n",
       "      <td>2016.09.17</td>\n",
       "      <td>5990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.09.15</td>\n",
       "      <td>16-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Bells Beach</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>2 m shark</td>\n",
       "      <td>The Age, 9/16/2016</td>\n",
       "      <td>2016.09.16-BellsBeach.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2016.09.16</td>\n",
       "      <td>2016.09.15</td>\n",
       "      <td>5989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case Number       Date  Year        Type    Country      Area  \\\n",
       "0  2016.09.18.c  18-Sep-16  2016  Unprovoked        USA   Florida   \n",
       "1  2016.09.18.b  18-Sep-16  2016  Unprovoked        USA   Florida   \n",
       "2  2016.09.18.a  18-Sep-16  2016  Unprovoked        USA   Florida   \n",
       "3    2016.09.17  17-Sep-16  2016  Unprovoked  AUSTRALIA  Victoria   \n",
       "4    2016.09.15  16-Sep-16  2016  Unprovoked  AUSTRALIA  Victoria   \n",
       "\n",
       "                           Location Activity             Name Sex   ...  \\\n",
       "0  New Smyrna Beach, Volusia County  Surfing             male    M  ...   \n",
       "1  New Smyrna Beach, Volusia County  Surfing   Chucky Luciano    M  ...   \n",
       "2  New Smyrna Beach, Volusia County  Surfing             male    M  ...   \n",
       "3                  Thirteenth Beach  Surfing  Rory Angiolella    M  ...   \n",
       "4                       Bells Beach  Surfing             male    M  ...   \n",
       "\n",
       "    Species        Investigator or Source                        pdf  \\\n",
       "0        NaN  Orlando Sentinel, 9/19/2016       2016.09.18.c-NSB.pdf   \n",
       "1        NaN  Orlando Sentinel, 9/19/2016   2016.09.18.b-Luciano.pdf   \n",
       "2        NaN  Orlando Sentinel, 9/19/2016       2016.09.18.a-NSB.pdf   \n",
       "3        NaN           The Age, 9/18/2016  2016.09.17-Angiolella.pdf   \n",
       "4  2 m shark           The Age, 9/16/2016  2016.09.16-BellsBeach.pdf   \n",
       "\n",
       "                                        href formula  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "\n",
       "                                                href Case Number.1  \\\n",
       "0  http://sharkattackfile.net/spreadsheets/pdf_di...  2016.09.18.c   \n",
       "1  http://sharkattackfile.net/spreadsheets/pdf_di...  2016.09.18.b   \n",
       "2  http://sharkattackfile.net/spreadsheets/pdf_di...  2016.09.18.a   \n",
       "3  http://sharkattackfile.net/spreadsheets/pdf_di...    2016.09.17   \n",
       "4  http://sharkattackfile.net/spreadsheets/pdf_di...    2016.09.16   \n",
       "\n",
       "  Case Number.2 original order Unnamed: 22 Unnamed: 23  \n",
       "0  2016.09.18.c           5993         NaN         NaN  \n",
       "1  2016.09.18.b           5992         NaN         NaN  \n",
       "2  2016.09.18.a           5991         NaN         NaN  \n",
       "3    2016.09.17           5990         NaN         NaN  \n",
       "4    2016.09.15           5989         NaN         NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comenzamos con una perspectiva visual del dataset.\n",
    "\n",
    "dataoriginal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records dropped:  0\n"
     ]
    }
   ],
   "source": [
    "# Encontramos y eliminamos registros duplicados\n",
    "'''\n",
    "The first thing we will do is attempt to drop any duplicate records, considering all the columns \n",
    "we currently have in the data set. Pandas provides us with the ability to do that via the \n",
    "drop_duplicates method. We will use the len method to calculate the number of rows in the data set \n",
    "both before and after removing duplicates and then print the number of rows dropped.\n",
    "'''\n",
    "before = len(dataoriginal)\n",
    "data = dataoriginal.drop_duplicates()\n",
    "after = len(dataoriginal)\n",
    "print('Number of duplicate records dropped: ', str(before - after))\n",
    "\n",
    "# En primera instancia data no presenta registros duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Reducimos data seleccionando solo las columnas interesantes en un nuevo DataFrame: data.\n",
    "Descartamos: Name, pdf, href formula, href, case number 1, Case numer 2, Unnamed 23, Unnamed 22.\n",
    "Las columnas descartadas no presentan información pertinente para los objetivos del proyecto.\n",
    "'''\n",
    "col_seleccion = ['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
    "                 'Activity', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time', 'Species ']\n",
    "data = dataoriginal[col_seleccion]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time', 'Species '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming columns\n",
    "\n",
    "'''\n",
    "Data will often come either without column names or with column names that are not as intuitive \n",
    "as they could be. When this is the case, we want to assign descriptive names to the columns so that \n",
    "we remember what the values in each column represent. \n",
    "'''\n",
    "\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Sex', 'Age', 'Injury', 'Fatal', 'Time', 'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambio de varias columnas.\n",
    "\n",
    "'''\n",
    "Cambiamos los nombres de las siguientes columnas:\n",
    "\n",
    "'Case number':'Case',\n",
    "'Sex ':'Sex'\n",
    "'Fatal (Y/N)': 'Fatal'\n",
    "'Species ': 'Sepecies'\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "data = data.rename(columns={'Case number':'Case',\n",
    "                            'Sex ':'Sex',\n",
    "                            'Fatal (Y/N)': 'Fatal',\n",
    "                            'Species ' :'Species'})\n",
    "\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Fatal', 'Injury', 'Type', 'Activity', 'Sex', 'Age',\n",
       "       'Date', 'Year', 'Country', 'Area', 'Location', 'Time', 'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambiando el orden de las columnas\n",
    "'''\n",
    "Cambiamor el orden de las columnas en función del objetivo del proyecto: localizar\n",
    "las playas con los tiburones más peligrosos para vender baños de alto riesgo a los \n",
    "amantes de la adrenalina\n",
    "'''\n",
    "\n",
    "column_order = ['Case Number', 'Fatal', 'Injury', 'Type', 'Activity', 'Sex', 'Age',  'Date', 'Year',  \n",
    "                'Country', 'Area', 'Location', 'Time', 'Species']\n",
    "\n",
    "data = data[column_order]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                           2116\n",
       "AUSTRALIA                     1279\n",
       "SOUTH AFRICA                   565\n",
       "PAPUA NEW GUINEA               133\n",
       "NEW ZEALAND                    125\n",
       "BRAZIL                         102\n",
       "BAHAMAS                         98\n",
       "MEXICO                          81\n",
       "ITALY                           71\n",
       "FIJI                            62\n",
       "PHILIPPINES                     59\n",
       "REUNION                         57\n",
       "NEW CALEDONIA                   51\n",
       "MOZAMBIQUE                      44\n",
       "CUBA                            42\n",
       "SPAIN                           40\n",
       "INDIA                           37\n",
       "EGYPT                           36\n",
       "CROATIA                         34\n",
       "PANAMA                          32\n",
       "JAPAN                           32\n",
       "IRAN                            29\n",
       "SOLOMON ISLANDS                 29\n",
       "GREECE                          25\n",
       "HONG KONG                       24\n",
       "JAMAICA                         23\n",
       "FRENCH POLYNESIA                22\n",
       "INDONESIA                       20\n",
       "ENGLAND                         19\n",
       "PACIFIC OCEAN                   17\n",
       "                              ... \n",
       "COOK ISLANDS                     1\n",
       "ICELAND                          1\n",
       "ANDAMAN / NICOBAR ISLANDAS       1\n",
       "RED SEA?                         1\n",
       "JAVA                             1\n",
       "BRITISH VIRGIN ISLANDS           1\n",
       "IRAN / IRAQ                      1\n",
       "BRITISH NEW GUINEA               1\n",
       "RED SEA / INDIAN OCEAN           1\n",
       "OCEAN                            1\n",
       "GABON                            1\n",
       "St Helena                        1\n",
       "MARTINIQUE                       1\n",
       "ARUBA                            1\n",
       "ADMIRALTY ISLANDS                1\n",
       "GUYANA                           1\n",
       "FALKLAND ISLANDS                 1\n",
       "KUWAIT                           1\n",
       "TASMAN SEA                       1\n",
       "BAHREIN                          1\n",
       "DIEGO GARCIA                     1\n",
       "Sierra Leone                     1\n",
       "ANGOLA                           1\n",
       "CURACAO                          1\n",
       "MONACO                           1\n",
       "NORTHERN MARIANA ISLANDS         1\n",
       "THE BALKANS                      1\n",
       " PHILIPPINES                     1\n",
       "PUERTO RICO                      1\n",
       "ST. MARTIN                       1\n",
       "Name: Country, Length: 203, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Fatal</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Type</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.09.18.c</td>\n",
       "      <td>N</td>\n",
       "      <td>Minor injury to thigh</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>16</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>13h00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.09.18.b</td>\n",
       "      <td>N</td>\n",
       "      <td>Lacerations to hands</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>11h00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.09.18.a</td>\n",
       "      <td>N</td>\n",
       "      <td>Lacerations to lower leg</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>43</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>10h43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.09.17</td>\n",
       "      <td>N</td>\n",
       "      <td>Struck by fin on chest &amp; leg</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Thirteenth Beach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.09.15</td>\n",
       "      <td>N</td>\n",
       "      <td>No injury: Knocked off board by shark</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Bells Beach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 m shark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case Number Fatal                                 Injury        Type  \\\n",
       "0  2016.09.18.c     N                  Minor injury to thigh  Unprovoked   \n",
       "1  2016.09.18.b     N                   Lacerations to hands  Unprovoked   \n",
       "2  2016.09.18.a     N               Lacerations to lower leg  Unprovoked   \n",
       "3    2016.09.17     N           Struck by fin on chest & leg  Unprovoked   \n",
       "4    2016.09.15     N  No injury: Knocked off board by shark  Unprovoked   \n",
       "\n",
       "  Activity Sex  Age       Date  Year    Country      Area  \\\n",
       "0  Surfing   M   16  18-Sep-16  2016        USA   Florida   \n",
       "1  Surfing   M   36  18-Sep-16  2016        USA   Florida   \n",
       "2  Surfing   M   43  18-Sep-16  2016        USA   Florida   \n",
       "3  Surfing   M  NaN  17-Sep-16  2016  AUSTRALIA  Victoria   \n",
       "4  Surfing   M  NaN  16-Sep-16  2016  AUSTRALIA  Victoria   \n",
       "\n",
       "                           Location   Time    Species  \n",
       "0  New Smyrna Beach, Volusia County  13h00        NaN  \n",
       "1  New Smyrna Beach, Volusia County  11h00        NaN  \n",
       "2  New Smyrna Beach, Volusia County  10h43        NaN  \n",
       "3                  Thirteenth Beach    NaN        NaN  \n",
       "4                       Bells Beach    NaN  2 m shark  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elección y creación de subset objeto de estudio\n",
    "\n",
    "'''\n",
    "When working with data, analysts often need to filter the data based on one or more conditional statements. \n",
    "\n",
    "We could enter our conditions inside square brackets to subset the data set for just the records that meet \n",
    "the conditions we’ve specified.\n",
    "\n",
    "'''\n",
    "# Comprobamos qué valores pueden contener las columnas 'Year' y 'Cuntry'\n",
    "\n",
    "data['Year'].unique() \n",
    "data['Country'].unique() \n",
    "\n",
    "# Comprobamos en qué años y países se dan más ataque\n",
    "data['Country'].value_counts()\n",
    "data['Year'].value_counts()\n",
    "\n",
    "# Realizamos el filtrado\n",
    "'''Seleccionamos los casos de los últimos 30 años, para tomar decisiones a partir de información actualizada, \n",
    "y los países con mayor concentración de casos, es decir, más de 360'''\n",
    "\n",
    "filtered = data[(data['Year']>= 1998) & \n",
    "                ((data['Country'] == 'USA') | (data['Country'] == 'AUSTRALIA') | (data['Country'] == 'SOUTH AFRICA'))]\n",
    "\n",
    "filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unprovoked      1214\n",
       "Invalid          104\n",
       "Provoked         102\n",
       "Boat              59\n",
       "Sea Disaster       2\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hector/.local/lib/python3.6/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/hector/.local/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Fatal</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Type</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Cause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.09.18.c</td>\n",
       "      <td>N</td>\n",
       "      <td>Minor injury to thigh</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>16</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>13h00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.09.18.b</td>\n",
       "      <td>N</td>\n",
       "      <td>Lacerations to hands</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>11h00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.09.18.a</td>\n",
       "      <td>N</td>\n",
       "      <td>Lacerations to lower leg</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>43</td>\n",
       "      <td>18-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>USA</td>\n",
       "      <td>Florida</td>\n",
       "      <td>New Smyrna Beach, Volusia County</td>\n",
       "      <td>10h43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.09.17</td>\n",
       "      <td>N</td>\n",
       "      <td>Struck by fin on chest &amp; leg</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Thirteenth Beach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.09.15</td>\n",
       "      <td>N</td>\n",
       "      <td>No injury: Knocked off board by shark</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-Sep-16</td>\n",
       "      <td>2016</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Bells Beach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 m shark</td>\n",
       "      <td>Natural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case Number Fatal                                 Injury        Type  \\\n",
       "0  2016.09.18.c     N                  Minor injury to thigh  Unprovoked   \n",
       "1  2016.09.18.b     N                   Lacerations to hands  Unprovoked   \n",
       "2  2016.09.18.a     N               Lacerations to lower leg  Unprovoked   \n",
       "3    2016.09.17     N           Struck by fin on chest & leg  Unprovoked   \n",
       "4    2016.09.15     N  No injury: Knocked off board by shark  Unprovoked   \n",
       "\n",
       "  Activity Sex  Age       Date  Year    Country      Area  \\\n",
       "0  Surfing   M   16  18-Sep-16  2016        USA   Florida   \n",
       "1  Surfing   M   36  18-Sep-16  2016        USA   Florida   \n",
       "2  Surfing   M   43  18-Sep-16  2016        USA   Florida   \n",
       "3  Surfing   M  NaN  17-Sep-16  2016  AUSTRALIA  Victoria   \n",
       "4  Surfing   M  NaN  16-Sep-16  2016  AUSTRALIA  Victoria   \n",
       "\n",
       "                           Location   Time    Species    Cause  \n",
       "0  New Smyrna Beach, Volusia County  13h00        NaN  Natural  \n",
       "1  New Smyrna Beach, Volusia County  11h00        NaN  Natural  \n",
       "2  New Smyrna Beach, Volusia County  10h43        NaN  Natural  \n",
       "3                  Thirteenth Beach    NaN        NaN  Natural  \n",
       "4                       Bells Beach    NaN  2 m shark  Natural  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Nos interesa conocer las causas del ataques: humanas o naturales.\n",
    "Para ello creamos una columna 'Cause' categorías significativas: natural, artificial o unknown'''\n",
    "\n",
    "'''\n",
    "Another way to create intuitive additional categories in your data is to create columns based on \n",
    "conditional statements. Earlier in this lesson, we filtered our data based on conditional statements. \n",
    "Here, we will populate the values in a column based on them using the loc method.\n",
    "\n",
    "Our vehicles data set currently has 45 different values in the Transmission field, but one of \n",
    "the key pieces of information embedded in there is whether a vehicle has an automatic or\n",
    "manual transmission. It would be valuable to extract that so that we could group vehicles by \n",
    "their transmission type. Let’s look at how we can create a new TransType column that only contains \n",
    "one of two values for each vehicle: Automatic or Manual.\n",
    "'''\n",
    "filtered.loc[filtered['Type'].str.startswith('U'), 'Cause'] = 'Natural'\n",
    "filtered.loc[filtered['Type'].str.startswith('P'), 'Cause'] = 'Artificial'\n",
    "filtered.loc[filtered['Type'].str.startswith('I'), 'Cause'] = 'Unknown'\n",
    "filtered.loc[filtered['Type'].str.startswith('B'), 'Cause'] = 'Unknown'\n",
    "filtered.loc[filtered['Type'].str.startswith('S'), 'Cause'] = 'Unknown'\n",
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fatal         1\n",
       "Injury        1\n",
       "Activity     58\n",
       "Sex          62\n",
       "Age         320\n",
       "Area          5\n",
       "Location     12\n",
       "Time        328\n",
       "Species     497\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analizammos el peso de los valores nulos en nuesttros datos\n",
    "'''\n",
    "Missing Values\n",
    "\n",
    "From this initial view, we can see that our data set contains some columns that have missing values \n",
    "in them and others that seem to have a lot of zero values. Let’s see how prevalent missing values are \n",
    "in our data. We can use the Pandas isnull method to check whether the value in each field is missing (null) \n",
    "and return either True or False for each field. We can use the sum method to total up the number of \n",
    "True values by column, and then we can add a condition using square brackets that will filter the data \n",
    "and show us only columns where the number of null values were greater than zero.\n",
    "'''\n",
    "null_cols = filtered.isnull().sum()\n",
    "null_cols[null_cols > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por la preponderancia de su valores nulos y la escasa aportación al estudio eliminamos la columna 'Species'\n",
    "'''\n",
    "We can see that some columns have relatively few null values while others have tens of thousands of nulls. \n",
    "For fields that have a lot of null values, you will often have to make a judgement call. If you don’t \n",
    "think the information is going to be very useful to your analysis, then you would remove those columns \n",
    "from your data frame.\n",
    "'''\n",
    "drop_cols = list(null_cols[null_cols > 400].index)\n",
    "filtered = filtered.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Fatal' e 'Injury' tienen un valor nulo, veamos qué ha ocurrido.\n",
    "\n",
    "'''\n",
    "Of the columns that remain, it looks like the cylinders column and the displ column have a similar \n",
    "number of nulls. Perhaps they are missing for similar reasons. We can investigate this by subsetting \n",
    "the data set and looking at just the records where displ is null and just the columns we think will \n",
    "be informative in allowing us to determine a reason.\n",
    "'''\n",
    "\n",
    "null_displ = filtered[(filtered['Fatal'].isnull()==True)]\n",
    "null_displ = null_displ[['Injury', 'Activity', 'Sex', 'Age', 'Area','Location','Time']]\n",
    "null_displ['Injury']\n",
    "\n",
    "# Observamos que dicho valor de fatal debería ser 'N', lo cambiamos.\n",
    "\n",
    "filtered[(filtered['Fatal'].isnull()==True)] = 'N'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_displ = filtered[(filtered['Injury'].isnull()==True)]\n",
    "null_displ = null_displ[['Injury', 'Fatal', 'Activity', 'Sex', 'Age', 'Area','Location','Time']]\n",
    "null_displ\n",
    "\n",
    "# Por sus registros dicho record debe ser eliminado\n",
    "filtered = filtered.drop(filtered[(filtered['Injury'].isnull()==True)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>13h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>11h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>10h43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60s</td>\n",
       "      <td>15h15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51</td>\n",
       "      <td>14h30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>Late afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>Late afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>15h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>37</td>\n",
       "      <td>14h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>49</td>\n",
       "      <td>16h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>18</td>\n",
       "      <td>12h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>36</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>31</td>\n",
       "      <td>After noon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11</td>\n",
       "      <td>11h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>59</td>\n",
       "      <td>10h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>42</td>\n",
       "      <td>14h30 / 15h30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>24</td>\n",
       "      <td>13h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>13</td>\n",
       "      <td>P.M.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>16</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>26</td>\n",
       "      <td>14h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>20</td>\n",
       "      <td>09h55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>18</td>\n",
       "      <td>18h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>15</td>\n",
       "      <td>2 hours after Opperman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>16</td>\n",
       "      <td>Early afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>40</td>\n",
       "      <td>11h10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>14</td>\n",
       "      <td>14h30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>24</td>\n",
       "      <td>15h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>22</td>\n",
       "      <td>14h45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>50</td>\n",
       "      <td>09h30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>40</td>\n",
       "      <td>15h30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>33</td>\n",
       "      <td>17h30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>32</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>-1</td>\n",
       "      <td>09h35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>31</td>\n",
       "      <td>20h00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>-1</td>\n",
       "      <td>Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age                    Time\n",
       "0      16                   13h00\n",
       "1      36                   11h00\n",
       "2      43                   10h43\n",
       "3      -1                      -1\n",
       "4      -1                      -1\n",
       "5      -1                      -1\n",
       "6     60s                   15h15\n",
       "7      51                   14h30\n",
       "9      12          Late afternoon\n",
       "10     -1          Late afternoon\n",
       "11      9                      -1\n",
       "12     22                      -1\n",
       "13     25                   15h00\n",
       "14     37                   14h00\n",
       "16     49                   16h00\n",
       "18     21                      -1\n",
       "19     22                      -1\n",
       "22     -1                      -1\n",
       "23     43                      -1\n",
       "24     18                   12h00\n",
       "26     36                      -1\n",
       "28     31              After noon\n",
       "29     -1                      -1\n",
       "30      9                    1300\n",
       "31     11                   11h00\n",
       "32     -1                      -1\n",
       "35     -1                      -1\n",
       "36     59                   10h00\n",
       "37     -1                      -1\n",
       "38     42           14h30 / 15h30\n",
       "...   ...                     ...\n",
       "1947   11                      -1\n",
       "1948   -1                      -1\n",
       "1949   19                      -1\n",
       "1950   24                   13h00\n",
       "1952   13                    P.M.\n",
       "1953   47                      -1\n",
       "1954   16                 Morning\n",
       "1955   10                      -1\n",
       "1956   26                   14h00\n",
       "1957   20                   09h55\n",
       "1958   18                   18h00\n",
       "1959   -1                      -1\n",
       "1960   17                      -1\n",
       "1961   15  2 hours after Opperman\n",
       "1962   16         Early afternoon\n",
       "1963   40                   11h10\n",
       "1964   14                   14h30\n",
       "1965   24                   15h00\n",
       "1966   22                   14h45\n",
       "1968   50                  09h30 \n",
       "1969   16                      -1\n",
       "1972   40                   15h30\n",
       "1973   33                   17h30\n",
       "1974   32                 Morning\n",
       "1975   -1                   09h35\n",
       "1976   31                   20h00\n",
       "1979   21                      -1\n",
       "1980   -1               Afternoon\n",
       "1982   28                      -1\n",
       "1984   -1                      -1\n",
       "\n",
       "[1480 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' El resto de valores nulos los sustituimos por ceros, \n",
    "excepto en Age y Time que sustituimos por -1 para evitar errores de interpretación.'''\n",
    "\n",
    "filtered[['Activity', 'Sex', 'Area', 'Location']].fillna(0)\n",
    "filtered[['Age', 'Time']].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N          1373\n",
       "Y           106\n",
       "UNKNOWN       1\n",
       "Name: Fatal, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.Fatal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evitamos valores incorrectos, estableciendo que en accidentes fatales, Injury marque High\n",
    "\n",
    "filtered.loc[(filtered['Fatal']== 'Y') & (filtered['Injury']!= 'No injury'), 'Injury'] = 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos que no hay columnas con low variance\n",
    "\n",
    "low_variance = []\n",
    "\n",
    "for col in filtered._get_numeric_data():\n",
    "    minimum = min(filtered[col])\n",
    "    ninety_perc = np.percentile(filtered[col], 90)\n",
    "    if ninety_perc == minimum:\n",
    "        low_variance.append(col)\n",
    "low_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot cut empty array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-7f8b99f8c3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m '''\n\u001b[1;32m     12\u001b[0m \u001b[0mage_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'old'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Adult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'young'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_numeric_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mage_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msz\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot cut empty array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot cut empty array"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Next, we must determine how we want our data to be binned. There are three main approaches that we can choose from:\n",
    "\n",
    "    Equal width bins: the range for each bin is the same size.\n",
    "    Equal frequency bins: approximately the same number of records in each bin.\n",
    "    Custom-sized bins: the user explicitly defines where they want the cutoff for each bin to be.\n",
    "\n",
    "If you want equal width bins, you can use the cut method \n",
    "\n",
    "'''\n",
    "\n",
    "bins = pd.cut(data['Combined MPG'],5, labels=mpg_labels)\n",
    "bins.head(10)\n",
    "\n",
    "# Equal frequency bins\n",
    "\n",
    "bins = pd.qcut(data['Combined MPG'],5, labels=mpg_labels)\n",
    "bins.head(10)\n",
    "\n",
    "'''\n",
    "Note the difference in results. With equal width binning, there will be some bins that contain more \n",
    "records than others (such as the Low bin). With equal frequency binning, some of those records will \n",
    "be forced into other bins (e.g. the Moderate bin and even the High bin). This is an important consideration \n",
    "when determining how you want to categorize your data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Finally, if you want custom bin sizes, you can pass a list of bin range values to the cut method instead of the number of bins, and it will bin the values for you accordingly.\n",
    "'''\n",
    "\n",
    "cutoffs = [7,14,21,23,30,40]\n",
    "bins = pd.cut(data['Combined MPG'],cutoffs, labels=mpg_labels)\n",
    "bins.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case Number    object\n",
       "Fatal          object\n",
       "Injury         object\n",
       "Type           object\n",
       "Activity       object\n",
       "Sex            object\n",
       "Age            object\n",
       "Date           object\n",
       "Year           object\n",
       "Country        object\n",
       "Area           object\n",
       "Location       object\n",
       "Time           object\n",
       "Cause          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data type correction\n",
    "'''\n",
    "This typically occurs when there is a numeric variable that should actually be represented as \n",
    "a categorical variable. \n",
    "'''\n",
    "filtered.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'N'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-8d40b2094661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m '''\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfiltered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[1;32m   5679\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5680\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[0;32m-> 5681\u001b[0;31m                                          **kwargs)\n\u001b[0m\u001b[1;32m   5682\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                                             copy=align_copy)\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[0;32m--> 534\u001b[0;31m                             **kwargs)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_astype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;31m# TODO(extension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'N'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Pandas currently has the year column stored as integers, but what if we wanted the year to be stored \n",
    "as a categorical variable (object) instead? \n",
    "'''\n",
    "filtered['Year'] = filtered['Year'].astype('int')\n",
    "data['year'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8', '62', '39', '50', '30', '58', '15', '48', '32', '5', '25', '24', '-14', '75', '43', '59', '36', '37', '21', '3', '11', '34', '26', '86', '14', '10', '9', '44', '40', '31', '19', '68', '33', '38', '64', '20', '53', '-1', '56', '77', '55', '63', '49', '16', '46', '51', '29', '23', '47', '35', '66', '41', '74', '13', '52', '28', '70', '22', '54', '65', '61', '84', '69', '71', '45', '6', '17', '12', '57', '7', '60', '73', '42', '18', '27'}\n"
     ]
    }
   ],
   "source": [
    "filtered['Age'] = filtered['Age'].fillna('-1')\n",
    "filtered['Age'] = filtered['Age'].str.replace('\\xa0 ', '-1')\n",
    "filtered['Age'] = filtered['Age'].str.replace('Teens', '14')\n",
    "filtered['Age'] = filtered['Age'].str.replace('36 & 26', '30')\n",
    "filtered['Age'] = filtered['Age'].str.replace('50s', '55')\n",
    "filtered['Age'] = filtered['Age'].str.replace('40s', '45')\n",
    "filtered['Age'] = filtered['Age'].str.replace('50s', '55')\n",
    "filtered['Age'] = filtered['Age'].str.replace('30s', '35')\n",
    "filtered['Age'] = filtered['Age'].str.replace('60s', '65')\n",
    "filtered['Age'] = filtered['Age'].str.replace('mid-30s', '35')\n",
    "filtered['Age'] = filtered['Age'].str.replace('8 or 10', '9')\n",
    "filtered['Age'] = filtered['Age'].str.replace('6œ', '-1')\n",
    "filtered['Age'] = filtered['Age'].str.replace('20s', '25')\n",
    "filtered['Age'] = filtered['Age'].str.replace('12 or 13', '12')\n",
    "filtered['Age'] = filtered['Age'].str.replace('33 or 37', '35')\n",
    "filtered['Age'] = filtered['Age'].str.replace('30 or 36', '33')\n",
    "filtered['Age'] = filtered['Age'].str.replace('teen', '14')\n",
    "filtered['Age'] = filtered['Age'].str.replace('Teen', '14')\n",
    "filtered['Age'] = filtered['Age'].str.replace('23 & 20', '21')\n",
    "filtered['Age'] = filtered['Age'].str.replace('N', '-1')\n",
    "print(set(filtered['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "So let’s remove all hyphens from this column with the help of the str.replace method and then print unique \n",
    "values again to ensure they were removed.\n",
    "\n",
    "'''\n",
    "data['trany'] = data['trany'].str.replace('-', '')\n",
    "print(set(data['trany']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You will also notice that in some cases Automatic is abbreviated to Auto and in other cases it is spelled out.\n",
    "We can make that more consistent by using the same technique. While we are at it, let’s also attempt \n",
    "to remove parentheses and make spacing more consistent.\n",
    "'''\n",
    "data['trany'] = data['trany'].str.replace('Automatic', 'Auto')\n",
    "data['trany'] = data['trany'].str.replace('Auto\\(', 'Auto ')\n",
    "data['trany'] = data['trany'].str.replace('Manual\\(', 'Manual ')\n",
    "data['trany'] = data['trany'].str.replace('\\(', '')\n",
    "data['trany'] = data['trany'].str.replace('\\)', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "As you can see, we now have no special characters, consistent naming, and proper spacing. We started out with 47 unique values in this column, and using this technique, we were able to reduce the number of unique values to 39.\n",
    "\n",
    "'''\n",
    "print(set(data['trany']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segunda búsqueda y eliminación de registros duplicados\n",
    "'''\n",
    "The first thing we will do is attempt to drop any duplicate records, considering all the columns \n",
    "we currently have in the data set. Pandas provides us with the ability to do that via the \n",
    "drop_duplicates method. We will use the len method to calculate the number of rows in the data set \n",
    "both before and after removing duplicates and then print the number of rows dropped.\n",
    "'''\n",
    "before = len(data)\n",
    "data = data.drop_duplicates()\n",
    "after = len(data)\n",
    "print('Number of duplicate records dropped: ', str(before - after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total price of all houses sold\n",
    "house_df['SalePrice'].sum()\n",
    "\n",
    "1901400\n",
    "\n",
    "# Average lot size of houses sold\n",
    "house_df['LotSize'].mean()\n",
    "\n",
    "10123.1\n",
    "\n",
    "# The latest year a house in the data set was built\n",
    "house_df['YearBuilt'].max()\n",
    "\n",
    "2004\n",
    "\n",
    "# The eariliest year a house in the data set was built\n",
    "house_df['YearBuilt'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de subsets: merge\n",
    "\n",
    "'''\n",
    "Another useful thing to do with data sets is to combine them. Pandas provides us with a few different \n",
    "ways to do this. The first way is by merging. Merging is similar to creating a join in SQL, \n",
    "where you can specify common fields between the two tables and then include information from both \n",
    "in your query. Pandas has a merge method that functions in a similar way.\n",
    "\n",
    "To illustrate, let’s create a data frame that has the average Combined MPG for each Make using the \n",
    "groupby method. We will merge that average into our data frame, joining on Make, so that we can see \n",
    "how fuel efficient a vehicle is in comparison to the other vehicles made by the same manufacturer.\n",
    "\n",
    "'''\n",
    "\n",
    "avg_mpg = data.groupby('Make', as_index=False).agg({'Combined MPG':'mean'})\n",
    "avg_mpg.columns = ['Make', 'Avg_MPG']\n",
    "\n",
    "data = pd.merge(data, avg_mpg, on='Make')\n",
    "data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de subsets: concat\n",
    "\n",
    "'''\n",
    "The pandas concat method lets you attach columns or rows from one data set onto another data set as \n",
    "long as both data sets have the same number of rows (if you are concatenating columns) or columns \n",
    "(if you are concatenating rows). Let’s take a look at examples for each of these.\n",
    "\n",
    "For column concatenation, we can use the one-hot encoded drivetrain data frame we created earlier \n",
    "and add those columns to our vehicles data set. Note that the data frames passed to the concat \n",
    "method must be in a list and you set the axis parameter to 1 in order to indicate that you are \n",
    "concatenating columns.\n",
    "'''\n",
    "data = pd.concat([data, drivetrain], axis=1)\n",
    "'''\n",
    "To illustrate row concatenation, let’s create two new data frames based on conditional filters \n",
    "from our original data frame - one containing only Lexus vehicles and another containing only Audi vehicles. \n",
    "We will then combine them using the concat method into a lexus_audi data frame that contains only vehicles \n",
    "manufactured by those two companies.\n",
    "'''\n",
    "lexus = data[data['Make']=='Lexus']\n",
    "audi = data[data['Make']=='Audi']\n",
    "\n",
    "lexus_audi = pd.concat([lexus, audi], axis=0)\n",
    "'''\n",
    "Again, note that the data frames are passed as a list and that this time the axis is set to 0 to specify that we are concatenating rows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de subset: agrupando por variable\n",
    "\n",
    "'''\n",
    "Our vehicles data set currently has a wide format, where there is a column for each attribute. \n",
    "However, some analytic and visualization tasks will require that the data be in a long format, \n",
    "where there are a few variables that define the entities and then all other attribute information \n",
    "is condensed into two columns: one containing the column/attribute names and another containing the value \n",
    "for that attribute for each entity. Pandas makes it easy to format data this way with the melt function. \n",
    "For example, suppose we were going to perform some analysis or visualization task where we needed the Year, \n",
    "Make, and Model to identify the vehicles and then we also needed the City MPG, Highway MPG, and Combined MPG \n",
    "fields for performing various calculations. Below is how we would melt the data into the proper format.\n",
    "\n",
    "'''\n",
    "\n",
    "melted = pd.melt(data, id_vars=['Year','Make','Model'], \n",
    "                 value_vars=['City MPG','Highway MPG','Combined MPG'])\n",
    "melted.head(10)\n",
    "\n",
    "'''\n",
    "As you can see, the column names have been stacked into the the variable field and their \n",
    "corresponding values have been stacked into the value field.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting DataFrame\n",
    "\n",
    "# Export comma-separated variable file\n",
    "data = pd.to_csv('vehicles/vehicles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
