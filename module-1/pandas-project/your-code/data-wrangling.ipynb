{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Título del proyecto\n",
    "\n",
    "'''\n",
    "\n",
    "Shark adventures: ¿Quieres sentir la adrenalina de nadar con tiburones?\n",
    "\n",
    "'''\n",
    "\n",
    "# Objetivo del proyecto\n",
    "\n",
    "'''\n",
    "Encontra la ubicación idónea para aquellas que quieren vivir experiencias extremas\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la importaciones que necesitaremos a lo largo del proyecto\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el archivo csv\n",
    "\n",
    "data = pd.read_csv('GSAF5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guía de trabajo del proyecto\n",
    "'''\n",
    "One of the first things we want to do is examine the data and look for any potential issues. Some of the things we are interested in identifying in the data at this stage include:\n",
    "\n",
    "    Missing values\n",
    "    Special characters\n",
    "    Incorrect values\n",
    "    Extreme values or outliers\n",
    "    Duplicate records\n",
    "    Incorrect data types\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontramos y eliminamos registros duplicados\n",
    "'''\n",
    "The first thing we will do is attempt to drop any duplicate records, considering all the columns \n",
    "we currently have in the data set. Pandas provides us with the ability to do that via the \n",
    "drop_duplicates method. We will use the len method to calculate the number of rows in the data set \n",
    "both before and after removing duplicates and then print the number of rows dropped.\n",
    "'''\n",
    "before = len(data)\n",
    "data = data.drop_duplicates()\n",
    "after = len(data)\n",
    "print('Number of duplicate records dropped: ', str(before - after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con una perspectiva visual del dataset.\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "\n",
    "'''\n",
    "Data will often come either without column names or with column names that are not as intuitive \n",
    "as they could be. When this is the case, we want to assign descriptive names to the columns so that \n",
    "we remember what the values in each column represent. \n",
    "'''\n",
    "\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambio de varias columnas.\n",
    "\n",
    "data.columns = ['Manufacturer','Model','Year','Displacement',\n",
    "                'Cylinders','Transmission','Drivetrain',\n",
    "                'Vehicle Class','Fuel Type','Fuel Barrels/Year',\n",
    "                'City MPG','Highway MPG','Combined MPG',\n",
    "                'CO2 Emission Grams/Mile','Fuel Cost/Year']\n",
    "\n",
    "data.columns\n",
    "\n",
    "\n",
    "# Cambio de pocas columnas\n",
    "\n",
    "data = data.rename(columns={'Manufacturer':'Make',\n",
    "                            'Displacement':'Engine Displacement'})\n",
    "\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elección y creación de subset objeto de estudio\n",
    "\n",
    "'''\n",
    "When working with data, analysts often need to filter the data based on one or more conditional statements. \n",
    "\n",
    "We could enter our conditions inside square brackets to subset the data set for just the records that meet \n",
    "the conditions we’ve specified.\n",
    "\n",
    "'''\n",
    "\n",
    "filtered = data[(data['Make']=='Ford') & \n",
    "                (data['Cylinders']>=6) & \n",
    "                (data['Combined MPG'] < 18)]\n",
    "\n",
    "filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiando el orden de las columnas\n",
    "\n",
    "column_order = ['Year','Make','Model','Vehicle Class',\n",
    "                'Transmission','Drivetrain','Fuel Type',\n",
    "                'Cylinders','Engine Displacement','Fuel Barrels/Year',\n",
    "                'City MPG','Highway MPG','Combined MPG',\n",
    "                'CO2 Emission Grams/Mile','Fuel Cost/Year']\n",
    "\n",
    "data = data[column_order]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columnas con categorías condicionales significativas: manual o automática\n",
    "'''\n",
    "Another way to create intuitive additional categories in your data is to create columns based on conditional statements. Earlier in this lesson, we filtered our data based on conditional statements. Here, we will populate the values in a column based on them using the loc method.\n",
    "\n",
    "Our vehicles data set currently has 45 different values in the Transmission field, but one of the key pieces of information embedded in there is whether a vehicle has an automatic or manual transmission. It would be valuable to extract that so that we could group vehicles by their transmission type. Let’s look at how we can create a new TransType column that only contains one of two values for each vehicle: Automatic or Manual.\n",
    "'''\n",
    "data.loc[data['Transmission'].str.startswith('A'), 'TransType'] = 'Automatic'\n",
    "data.loc[data['Transmission'].str.startswith('M'), 'TransType'] = 'Manual'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding categoriacal variables\n",
    "\n",
    "'''\n",
    "This is useful for performing analyses where you want to know whether something falls into a specific \n",
    "category or not. It will also be useful when you learn about machine learning, as one-hot encoding makes \n",
    "it easier for some algorithms to interpret and find patterns in categorical data.\n",
    "\n",
    "To perform one-hot encoding on a column, you can use the Pandas get_dummies method and \n",
    "pass it the column you would like to one-hot encode.\n",
    "\n",
    "'''\n",
    "drivetrain = pd.get_dummies(data['Drivetrain'])\n",
    "drivetrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Missing Values\n",
    "\n",
    "From this initial view, we can see that our data set contains some columns that have missing values \n",
    "in them and others that seem to have a lot of zero values. Let’s see how prevalent missing values are \n",
    "in our data. We can use the Pandas isnull method to check whether the value in each field is missing (null) \n",
    "and return either True or False for each field. We can use the sum method to total up the number of \n",
    "True values by column, and then we can add a condition using square brackets that will filter the data \n",
    "and show us only columns where the number of null values were greater than zero.\n",
    "'''\n",
    "null_cols = data.isnull().sum()\n",
    "null_cols[null_cols > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We can see that some columns have relatively few null values while others have tens of thousands of nulls. \n",
    "For fields that have a lot of null values, you will often have to make a judgement call. If you don’t \n",
    "think the information is going to be very useful to your analysis, then you would remove those columns \n",
    "from your data frame.\n",
    "'''\n",
    "drop_cols = list(null_cols[null_cols > 10000].index)\n",
    "data = data.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining columns that have null values.\n",
    "\n",
    "'''\n",
    "Of the columns that remain, it looks like the cylinders column and the displ column have a similar \n",
    "number of nulls. Perhaps they are missing for similar reasons. We can investigate this by subsetting \n",
    "the data set and looking at just the records where displ is null and just the columns we think will \n",
    "be informative in allowing us to determine a reason.\n",
    "'''\n",
    "\n",
    "null_displ = data[(data['displ'].isnull()==True)]\n",
    "null_displ = null_displ[['year', 'make', 'model', 'trany', 'drive','fuelType','cylinders', 'displ']]\n",
    "null_displ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrategia para eliminar los null values.\n",
    "\n",
    "'''\n",
    "We can see that most of the time, cylinders is null when displ is null and that the most of the records \n",
    "where both fields are null have a fuel type of Electricity. This makes sense, as electric cars do not \n",
    "have cylinders and can therefore not have any displacement. \n",
    "In this case, it would make sense to replace these null values with zeros.\n",
    " there are other strategies for filling in nulls. Depending on the circumstances, you might want to \n",
    " replace nulls with the column mean or mode values.\n",
    "'''\n",
    "\n",
    "data[['displ', 'cylinders']] = data[['displ', 'cylinders']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect values\n",
    "\n",
    "'''\n",
    "in the previous section, we learned that a vehicle without cylinders should not have displacement \n",
    "and vice versa. Let’s check to see if there are any cases that violate these rules.\n",
    "'''\n",
    "\n",
    "test = data[(data['cylinders']==0) & (data['displ']!=0)]\n",
    "test[['year', 'make', 'model', 'trany', 'drive','fuelType','cylinders', 'displ']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "try to look at similar vehicles in the data set and determine the most likely value for this field.\n",
    "\n",
    "Suppose that using one of the aforementioned methods, we found out that this vehicle actually \n",
    "has a 4 cylinder engine. Once we have this information, we can use the loc method to update that \n",
    "specific value in the data frame.\n",
    "\n",
    "Challenge: Try to find other values that might be incorrect in the data set based on what you know about automobiles and correct them.\n",
    "'''\n",
    "\n",
    "data.loc[(data['cylinders']==0) & (data['displ']!=0), 'cylinders'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low variance column\n",
    "\n",
    "'''\n",
    "When analyzing data, we want the fields we are working with to be informative, and we will want to \n",
    "strip away any columns that don’t have a lot of value to us. One easy way to do this is to identify \n",
    "columns that have low variance, where the majority of the values in the column are the same. Since \n",
    "there is not a lot of variability in these columns, they have the potential to not be as informative \n",
    "as columns that have a variety of different values in them.\n",
    "\n",
    "Let’s try to identify columns where at least 90% of the values are the same so that we can remove them \n",
    "from our data set. \n",
    "'''\n",
    "\n",
    "low_variance = []\n",
    "\n",
    "for col in data._get_numeric_data():\n",
    "    minimum = min(data[col])\n",
    "    ninety_perc = np.percentile(data[col], 90)\n",
    "    if ninety_perc == minimum:\n",
    "        low_variance.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This returned 34 columns that we could potentially eliminate due to not having high enough variability \n",
    "to be informative. Of course, before we do this, we should check the values that do exist in these fields \n",
    "to confirm that they are not very informative. Once they have been checked, we can use the the drop method \n",
    "like we did earlier in this lesson to remove those columns from our data frame.\n",
    "'''\n",
    "data = data.drop(low_variance, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme values and outliers\n",
    "\n",
    "'''\n",
    "hese outliers can influence our aggregations when we are analyzing data later, so we want to make sure \n",
    "we address them during our data cleaning stage.\n",
    "\n",
    "A common method for identifying outliers is one that leverages the interquartile range (IQR). \n",
    "Once the IQR is calculated, it is multiplied by a constant (typically 1.5) and lower and upper bounds are \n",
    "established at:\n",
    "\n",
    "    25th Percentile - (IQR x 1.5)\n",
    "    75th Percentile + (IQR x 1.5)\n",
    "\n",
    "Any values outside this range are potential outliers and should be investigated.\n",
    "'''\n",
    "\n",
    "stats = data.describe().transpose()\n",
    "stats['IQR'] = stats['75%'] - stats['25%']\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will then create an empty data frame called outliers with the same columns as our data set. \n",
    "Finally, we will loop through each column in the data calculating the lower and upper bounds, \n",
    "retrieving records where the value for that column falls outside the bounds we established, \n",
    "and appending those results to our outlier data frame.\n",
    "\n",
    "outliers = pd.DataFrame(columns=data.columns)\n",
    "'''\n",
    "\n",
    "for col in stats.index:\n",
    "    iqr = stats.at[col,'IQR']\n",
    "    cutoff = iqr * 1.5\n",
    "    lower = stats.at[col,'25%'] - cutoff\n",
    "    upper = stats.at[col,'75%'] + cutoff\n",
    "    results = data[(data[col] < lower) | \n",
    "                   (data[col] > upper)].copy()\n",
    "    results['Outlier'] = col\n",
    "    outliers = outliers.append(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning numeric variables\n",
    "\n",
    "'''\n",
    "When preparing data to be analyzed, one of the things that is useful to do is to create additional \n",
    "categorical variables. Categorical variables allow you to group records in different ways, \n",
    "and each way that you categorize them can provide you with a different perspective when you’re \n",
    "conducting your analysis. A common way of creating additional categorical fields is to bin numeric \n",
    "variables in a column based on how relatively high or low they are.\n",
    "'''\n",
    "\n",
    "mpg_labels = ['Very Low', 'Low', 'Moderate', 'High', 'Very High']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, we must determine how we want our data to be binned. There are three main approaches that we can choose from:\n",
    "\n",
    "    Equal width bins: the range for each bin is the same size.\n",
    "    Equal frequency bins: approximately the same number of records in each bin.\n",
    "    Custom-sized bins: the user explicitly defines where they want the cutoff for each bin to be.\n",
    "\n",
    "If you want equal width bins, you can use the cut method \n",
    "\n",
    "'''\n",
    "\n",
    "bins = pd.cut(data['Combined MPG'],5, labels=mpg_labels)\n",
    "bins.head(10)\n",
    "\n",
    "# Equal frequency bins\n",
    "\n",
    "bins = pd.qcut(data['Combined MPG'],5, labels=mpg_labels)\n",
    "bins.head(10)\n",
    "\n",
    "'''\n",
    "Note the difference in results. With equal width binning, there will be some bins that contain more \n",
    "records than others (such as the Low bin). With equal frequency binning, some of those records will \n",
    "be forced into other bins (e.g. the Moderate bin and even the High bin). This is an important consideration \n",
    "when determining how you want to categorize your data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Finally, if you want custom bin sizes, you can pass a list of bin range values to the cut method instead of the number of bins, and it will bin the values for you accordingly.\n",
    "'''\n",
    "\n",
    "cutoffs = [7,14,21,23,30,40]\n",
    "bins = pd.cut(data['Combined MPG'],cutoffs, labels=mpg_labels)\n",
    "bins.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data type correction\n",
    "'''\n",
    "This typically occurs when there is a numeric variable that should actually be represented as \n",
    "a categorical variable. \n",
    "'''\n",
    "data.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pandas currently has the year column stored as integers, but what if we wanted the year to be stored \n",
    "as a categorical variable (object) instead? \n",
    "'''\n",
    "data['year'] = data['year'].astype('object')\n",
    "data['year'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In our vehicles data set, the trany field has several special characters (parentheses, hyphens, etc.). \n",
    "We can take a look at the unique values in this column by using the set function.\n",
    "'''\n",
    "print(set(data['trany']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "So let’s remove all hyphens from this column with the help of the str.replace method and then print unique \n",
    "values again to ensure they were removed.\n",
    "\n",
    "'''\n",
    "data['trany'] = data['trany'].str.replace('-', '')\n",
    "print(set(data['trany']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You will also notice that in some cases Automatic is abbreviated to Auto and in other cases it is spelled out.\n",
    "We can make that more consistent by using the same technique. While we are at it, let’s also attempt \n",
    "to remove parentheses and make spacing more consistent.\n",
    "'''\n",
    "data['trany'] = data['trany'].str.replace('Automatic', 'Auto')\n",
    "data['trany'] = data['trany'].str.replace('Auto\\(', 'Auto ')\n",
    "data['trany'] = data['trany'].str.replace('Manual\\(', 'Manual ')\n",
    "data['trany'] = data['trany'].str.replace('\\(', '')\n",
    "data['trany'] = data['trany'].str.replace('\\)', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "As you can see, we now have no special characters, consistent naming, and proper spacing. We started out with 47 unique values in this column, and using this technique, we were able to reduce the number of unique values to 39.\n",
    "\n",
    "'''\n",
    "print(set(data['trany']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segunda búsqueda y eliminación de registros duplicados\n",
    "'''\n",
    "The first thing we will do is attempt to drop any duplicate records, considering all the columns \n",
    "we currently have in the data set. Pandas provides us with the ability to do that via the \n",
    "drop_duplicates method. We will use the len method to calculate the number of rows in the data set \n",
    "both before and after removing duplicates and then print the number of rows dropped.\n",
    "'''\n",
    "before = len(data)\n",
    "data = data.drop_duplicates()\n",
    "after = len(data)\n",
    "print('Number of duplicate records dropped: ', str(before - after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total price of all houses sold\n",
    "house_df['SalePrice'].sum()\n",
    "\n",
    "1901400\n",
    "\n",
    "# Average lot size of houses sold\n",
    "house_df['LotSize'].mean()\n",
    "\n",
    "10123.1\n",
    "\n",
    "# The latest year a house in the data set was built\n",
    "house_df['YearBuilt'].max()\n",
    "\n",
    "2004\n",
    "\n",
    "# The eariliest year a house in the data set was built\n",
    "house_df['YearBuilt'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de subsets: merge\n",
    "\n",
    "'''\n",
    "Another useful thing to do with data sets is to combine them. Pandas provides us with a few different \n",
    "ways to do this. The first way is by merging. Merging is similar to creating a join in SQL, \n",
    "where you can specify common fields between the two tables and then include information from both \n",
    "in your query. Pandas has a merge method that functions in a similar way.\n",
    "\n",
    "To illustrate, let’s create a data frame that has the average Combined MPG for each Make using the \n",
    "groupby method. We will merge that average into our data frame, joining on Make, so that we can see \n",
    "how fuel efficient a vehicle is in comparison to the other vehicles made by the same manufacturer.\n",
    "\n",
    "'''\n",
    "\n",
    "avg_mpg = data.groupby('Make', as_index=False).agg({'Combined MPG':'mean'})\n",
    "avg_mpg.columns = ['Make', 'Avg_MPG']\n",
    "\n",
    "data = pd.merge(data, avg_mpg, on='Make')\n",
    "data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de subsets: concat\n",
    "\n",
    "'''\n",
    "The pandas concat method lets you attach columns or rows from one data set onto another data set as \n",
    "long as both data sets have the same number of rows (if you are concatenating columns) or columns \n",
    "(if you are concatenating rows). Let’s take a look at examples for each of these.\n",
    "\n",
    "For column concatenation, we can use the one-hot encoded drivetrain data frame we created earlier \n",
    "and add those columns to our vehicles data set. Note that the data frames passed to the concat \n",
    "method must be in a list and you set the axis parameter to 1 in order to indicate that you are \n",
    "concatenating columns.\n",
    "'''\n",
    "data = pd.concat([data, drivetrain], axis=1)\n",
    "'''\n",
    "To illustrate row concatenation, let’s create two new data frames based on conditional filters \n",
    "from our original data frame - one containing only Lexus vehicles and another containing only Audi vehicles. \n",
    "We will then combine them using the concat method into a lexus_audi data frame that contains only vehicles \n",
    "manufactured by those two companies.\n",
    "'''\n",
    "lexus = data[data['Make']=='Lexus']\n",
    "audi = data[data['Make']=='Audi']\n",
    "\n",
    "lexus_audi = pd.concat([lexus, audi], axis=0)\n",
    "'''\n",
    "Again, note that the data frames are passed as a list and that this time the axis is set to 0 to specify that we are concatenating rows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinación de subset: agrupando por variable\n",
    "\n",
    "'''\n",
    "Our vehicles data set currently has a wide format, where there is a column for each attribute. \n",
    "However, some analytic and visualization tasks will require that the data be in a long format, \n",
    "where there are a few variables that define the entities and then all other attribute information \n",
    "is condensed into two columns: one containing the column/attribute names and another containing the value \n",
    "for that attribute for each entity. Pandas makes it easy to format data this way with the melt function. \n",
    "For example, suppose we were going to perform some analysis or visualization task where we needed the Year, \n",
    "Make, and Model to identify the vehicles and then we also needed the City MPG, Highway MPG, and Combined MPG \n",
    "fields for performing various calculations. Below is how we would melt the data into the proper format.\n",
    "\n",
    "'''\n",
    "\n",
    "melted = pd.melt(data, id_vars=['Year','Make','Model'], \n",
    "                 value_vars=['City MPG','Highway MPG','Combined MPG'])\n",
    "melted.head(10)\n",
    "\n",
    "'''\n",
    "As you can see, the column names have been stacked into the the variable field and their \n",
    "corresponding values have been stacked into the value field.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting DataFrame\n",
    "\n",
    "# Export comma-separated variable file\n",
    "data = pd.to_csv('vehicles/vehicles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
