{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.metrics \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://elpais.com/internacional/2019/03/20/actualidad/1553092068_648405.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_scraper(url):\n",
    "    '''\n",
    "    Input: a string with the URL of the new\n",
    "    Output: a dictionary with thes key-value pairs:\n",
    "        'url': a string with the new's url \n",
    "        'h1': a string with the headline of the new\n",
    "        'h2': a string with the subtitle of the new\n",
    "        'author': a string with the author of the new\n",
    "        'body': a string with the body of the new\n",
    "    '''\n",
    "    \n",
    "    new_scraped = {}\n",
    "    html = requests.get(url).content\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    if 'elpais.com' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h1',{'itemprop':\"headline\"}))\n",
    "        new_scraped['h2'] = ''.join(element.text for element in soup.find_all('h2',{'itemprop':\"alternativeHeadline\"}))\n",
    "        new_scraped['author'] = ''.join(element.text.replace(\"\\n\",\"\") for element in soup.find_all('span',{'class':\"autor-nombre\"}))\n",
    "        new_scraped['body'] = ''.join(element.text.replace('\\n', '') for element in soup.find_all('p')).split('NEWSLETTER')[0]\n",
    "        \n",
    "        \n",
    "    elif 'elmundo.es' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h1'))\n",
    "        new_scraped['h2'] = ''.join(element.text for element in soup.find_all('p',{'class':\"ue-c-article__standfirst\"}))\n",
    "        new_scraped['author'] = ''.join(element.text.replace(\"\\n\",\"\") for element in soup.find_all('div',{'class':\"ue-c-article__byline-name\"}))\n",
    "        new_scraped['body'] = ''.join(element.text.replace('\\n', ' ') for element in soup.find_all('div',{'class':\"ue-l-article__body ue-c-article__body\"})).split('Conforme a los criterios deThe Trust Project')[0]\n",
    "        \n",
    "    \n",
    "    elif 'abc.es' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('span',{'class':\"titular\"}))\n",
    "        new_scraped['h2'] = ''.join(element.text for element in soup.find_all('h2',{'class':\"subtitulo\"}))\n",
    "        new_scraped['author'] = ''.join(element.text for element in soup.find_all('a',{'class':\"autor\"}))\n",
    "        new_scraped['body'] = ''.join(element.text.replace(\"''\", '') for element in soup.find_all('p')).split('¡Hola, !')[0]\n",
    "        \n",
    "        \n",
    "    elif 'lavozdegalicia.es' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h1',{'itemprop':\"headline\"}))\n",
    "        new_scraped['h2'] = ''.join(element.text for element in soup.find_all('h2',{'itemprop':\"alternativeHeadline\"}))\n",
    "        new_scraped['author'] = ''.join(element.text.replace('\\n','') for element in soup.find_all('span',{'class':\"author\"})).replace('\\t', '')\n",
    "        new_scraped['body'] = ''.join(element.text for element in soup.find_all('p')).split('Hemos creado para ti una selección de contenidos para que los recibas')[0]\n",
    "        \n",
    "    \n",
    "    elif 'lavanguardia.com' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h1',{'itemprop':\"headline\"}))\n",
    "        new_scraped['h2'] = ''.join(element.text for element in soup.find_all('h2',{'itemprop':\"alternativeHeadline\"}))\n",
    "        new_scraped['author'] = ''.join(element.text for element in soup.find_all('span',{'itemprop':\"name\"}))\n",
    "        new_scraped['body'] = ''.join(element.text.replace('\\n', '') for element in soup.find_all('div',{'itemprop':\"articleBody\"}))\n",
    "       \n",
    "  \n",
    "    elif 'elmundotoday.com' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h1',{'class':\"entry-title\"}))\n",
    "        new_scraped['h2'] = ''.join(element.text for element in soup.find_all('p',{'class':\"td-post-sub-title\"}))\n",
    "        new_scraped['author'] = ''.join(element.text.replace('Por', '') for element in soup.find_all('div',{'class':\"td-post-author-name\"}))\n",
    "        new_scraped['body'] = ''.join(element.text.replace('\\n', '') for element in soup.find_all('div',{'class':\"td-pb-span10\"}))\n",
    "        \n",
    "    \n",
    "    elif 'alertadigital.com' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h2'))\n",
    "        new_scraped['h2'] = None\n",
    "        new_scraped['author'] = ''.join(element.text for element in soup.find_all('div',{'id':\"datemeta_r\"})).split('|')[0]\n",
    "        new_scraped['body'] = ''.join(element.text.replace('\\n', '') for element in soup.find_all('div',{'class':\"entry\"})).replace('\\xa0', '')\n",
    "    \n",
    "    \n",
    "    elif 'okdiario.com' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h1',{'class':\"entry-title\"}))\n",
    "        new_scraped['h2'] = ''.join(element.text for element in soup.find_all('h2',{'itemprop':\"alternativeHeadline\"}))\n",
    "        new_scraped['author'] = ''.join(element.text.replace('\\n', '') for element in soup.find_all('li',{'class':\"author-name\"}))\n",
    "        new_scraped['body'] = ''.join(element.text.replace('\\n', '') for element in soup.find_all('div',{'class':\"entry-content\"}))\n",
    "\n",
    "    \n",
    "    elif 'mediterraneodigital.com' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text.replace('\\t', '') for element in soup.find_all('h2',{'class':\"contentheading\"})).replace('\\n', '')\n",
    "        new_scraped['h2'] = None\n",
    "        new_scraped['author'] = ''.join(element.text.replace('\\t', '') for element in soup.find_all('dd',{'class':\"createdby\"})).replace('Escrito por', '').replace('\\n', '')\n",
    "        new_scraped['body'] = (''.join(element.text for element in soup.find_all('p',{'style':\"text-align: justify;\"}))).split('\\xa0©\\xa0GRUP')[0]\n",
    "    \n",
    "    \n",
    "    elif 'periodistadigital.com' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h1'))\n",
    "        new_scraped['h2'] = ''.join(element.text for element in soup.find_all('p',{'class':\"subtitle\"}))\n",
    "        new_scraped['author'] = (''.join(element.text for element in soup.find_all('div',{'class':\"page-header-author text-left\"}))).split(',')[0].replace('\\n', '')\n",
    "        new_scraped['body'] = ''.join(element.text for element in soup.find_all('div',{'class':\"text-block\"})).replace('\\n', '')\n",
    "   \n",
    "  \n",
    "    elif 'haynoticia.es' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text.replace('\\n', '') for element in soup.find_all('h1',{'class':\"entry-title\"})).replace('\\t', '')\n",
    "        new_scraped['h2'] = None\n",
    "        new_scraped['author'] = ''.join(element.text.replace(\"\\n\",\"\") for element in soup.find_all('span',{'class':\"author vcard\"}))\n",
    "        new_scraped['body'] = ''.join(element.text.replace('\\xa0', ' ') for element in soup.find_all('p')).replace('No creas todo lo que lees por Internet', '')\n",
    "\n",
    "    \n",
    "    elif 'thepatriota.com' in url:\n",
    "        new_scraped['url'] = url\n",
    "        new_scraped['h1'] = ''.join(element.text for element in soup.find_all('h1',{'class':\"entry-title\"}))\n",
    "        new_scraped['h2'] = None\n",
    "        new_scraped['author'] = ''.join(element.text.replace('Por','') for element in soup.find_all('div',{'class':\"td-post-author-name\"})).replace('-', '')\n",
    "        new_scraped['body'] = ''.join(element.text for element in soup.find_all('p')).split('Nuestra web es un sitio de humor')[0]\n",
    "   \n",
    "              \n",
    "    \n",
    "    return new_scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_text(text):\n",
    "    \"\"\"\n",
    "    The function cleans up numbers, remove punctuation and line break, and special characters from a string \n",
    "    and converts it to lowercase.\n",
    "\n",
    "    Args:\n",
    "        text: The string to be cleaned up.\n",
    "\n",
    "    Returns:\n",
    "        A string that has been cleaned up.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text) \n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    text = re.sub('[‘’“”…«»¿?¡!\\-_\\(\\)]', '', text)\n",
    "    text = re.sub(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', text)\n",
    "  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = 'finalized_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vectorizer from disk\n",
    "filename = 'finalized_vectorizer.sav'\n",
    "loaded_vectorizer = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scraped = news_scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = [clean_up_text(new_scraped['body'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = loaded_vectorizer.transform(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
